{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "73bSrkGkHDUj",
        "outputId": "26d89bc4-b27f-45e3-8f30-2d541331c3a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Email_Intent_recognition'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 43 (delta 12), reused 35 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (43/43), 1021.06 KiB | 3.19 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nik-su/Email_Intent_recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VJUzlOX1HedW",
        "outputId": "0052109c-474b-4bd5-86e0-113b7542b541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Email_Intent_recognition\n",
            " app.py\t\t\t\t    model_training.py\n",
            "'Dataset Preparation Summary.pdf'  'Model Training Summary.pdf'\n",
            " dataset.py\t\t\t    __pycache__\n",
            " email_intent_dataset.csv\t    Readme.md\n",
            " Email_Intent_Run.ipynb\t\t    requirements.txt\n",
            " evaluation_model.py\t\t    save_to_colab.py\n",
            " Inference_test.py\n"
          ]
        }
      ],
      "source": [
        "%cd Email_Intent_recognition/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tEXas31aHjE1",
        "outputId": "aac8555a-d06e-4ccb-9df7-1fa18ac26273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.51.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Collecting fastapi (from -r requirements.txt (line 8))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn (from -r requirements.txt (line 9))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pyngrok (from -r requirements.txt (line 10))\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nlpaug (from -r requirements.txt (line 12))\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
            "Collecting textaugment (from -r requirements.txt (line 17))\n",
            "  Downloading textaugment-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.8.5)\n",
            "Collecting sacremoses (from -r requirements.txt (line 20))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (4.67.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.31.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->-r requirements.txt (line 8))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi->-r requirements.txt (line 8)) (2.11.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 9)) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug->-r requirements.txt (line 12)) (5.2.0)\n",
            "Collecting gensim>=4.0 (from textaugment->-r requirements.txt (line 17))\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (from textaugment->-r requirements.txt (line 17)) (0.19.0)\n",
            "Collecting googletrans>=2 (from textaugment->-r requirements.txt (line 17))\n",
            "  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (0.15.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 18)) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 23)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 23)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 23)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 23)) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 23)) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 23)) (1.20.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug->-r requirements.txt (line 12)) (4.13.4)\n",
            "Collecting numpy (from -r requirements.txt (line 6))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 7))\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim>=4.0->textaugment->-r requirements.txt (line 17)) (7.1.0)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans>=2->textaugment->-r requirements.txt (line 17)) (0.28.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 8)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2025.4.26)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi->-r requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 18)) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy->-r requirements.txt (line 18))\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 18))\n",
            "  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 18)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 18)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 18)) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans>=2->textaugment->-r requirements.txt (line 17)) (1.0.9)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans>=2->textaugment->-r requirements.txt (line 17)) (4.2.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 18)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 18)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 18)) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim>=4.0->textaugment->-r requirements.txt (line 17)) (1.17.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug->-r requirements.txt (line 12)) (2.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug->-r requirements.txt (line 12)) (1.7.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans>=2->textaugment->-r requirements.txt (line 17)) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans>=2->textaugment->-r requirements.txt (line 17)) (4.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 18)) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textaugment-2.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
            "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, sacremoses, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, starlette, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, blis, nvidia-cusolver-cu12, gensim, fastapi, thinc, nlpaug, googletrans, textaugment\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.2.1 fastapi-0.115.12 gensim-4.3.3 googletrans-4.0.2 nlpaug-1.1.11 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyngrok-7.2.8 sacremoses-0.1.1 scipy-1.13.1 starlette-0.46.2 textaugment-2.0.0 thinc-8.3.4 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ILKvcwWwHzZc",
        "outputId": "7d37a0d7-98ec-423d-9fc7-cd9644f7e4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l6A-UDQMHoy8",
        "outputId": "f65f3d83-a925-40ea-ba0d-0df58a9c676f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-12 15:52:53.689023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747065173.710245    6156 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747065173.716416    6156 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-12 15:52:53.738686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Running in Google Colab - mounting Google Drive...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Email_Intent_recognition/model_training.py\", line 354, in <module>\n",
            "    drive.mount('/content/drive')\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 100, in mount\n",
            "    return _mount(\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 137, in _mount\n",
            "    _message.blocking_request(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 173, in blocking_request\n",
            "    request_id = send_request(\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 117, in send_request\n",
            "    instance = ipython.get_kernelapp()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_ipython.py\", line 28, in get_kernelapp\n",
            "    return get_ipython().kernel.parent\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'kernel'\n"
          ]
        }
      ],
      "source": [
        "!python model_training.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsIOhdyjNMh8",
        "outputId": "be8a1526-6068-43be-aa9d-7b667f15e60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy.rec (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy.rec\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy.rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5b073226c8194df8b2fe74fd2418184c",
            "cfab7bf25cb04af99cb8a640eeda571d",
            "08a53fa6786944629d60a2937172a370",
            "f9f42e7d88d84ccdbab419720268099d",
            "cf0f2721567448a0b1740ae3ea046671",
            "7ab13dd36aff41a992b55970439a39ad",
            "f64cff16376440a28e4c85b277b8366b",
            "f66d41fc8cb142309f1d4d34ca9d710e",
            "57edf8af70684220ba96e521cf9c1082",
            "da1c89788ee9412499de080d6aa67d8b",
            "9e4f8e33dc0743baa56efe35c7523c50",
            "a643e5c82ff04cdcbdae012ee94f55dc",
            "5f296bbb606a44b3bfddc0bc38dcf252",
            "bf54cc60cbc840a79bcac2154564357e",
            "3a9b6f271082492b9703fe6ddda2b850",
            "f97b9acc98224159a190399a3c2a1bd4",
            "ec2666c4a37540e08b9edea94c0826e0",
            "77fe1ae1e8344882ad929da9835fc765",
            "a25983eb77c1424188a833c2c1235e67",
            "79b684c4651340daa837e5ab4bb362f3",
            "fd129c2e3da94774a9736a16e8c4721d",
            "9da1159d7c8a41bebe061b58e03db0b4"
          ]
        },
        "id": "4S7FOOQrNBFC",
        "outputId": "f41aec36-fb48-4689-d4e7-f0e6c6300012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Loading dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b073226c8194df8b2fe74fd2418184c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a643e5c82ff04cdcbdae012ee94f55dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-6-217efb10ec6c>:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikhilforllms\u001b[0m (\u001b[33mnikhilforllms-minfy\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/Email_Intent_recognition/wandb/run-20250512_160258-e01wwisn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nikhilforllms-minfy/huggingface/runs/e01wwisn' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nikhilforllms-minfy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nikhilforllms-minfy/huggingface' target=\"_blank\">https://wandb.ai/nikhilforllms-minfy/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nikhilforllms-minfy/huggingface/runs/e01wwisn' target=\"_blank\">https://wandb.ai/nikhilforllms-minfy/huggingface/runs/e01wwisn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 1:02:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.487800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "                                                          precision    recall  f1-score   support\n",
            "\n",
            "                            Intent_Amendment_Abstraction       1.00      1.00      1.00       143\n",
            "                                   Intent_Clause_Protect       1.00      1.00      1.00       123\n",
            "                                 Intent_Company_research       1.00      1.00      1.00       132\n",
            "           Intent_Company_research;Intent_Clause_Protect       1.00      1.00      1.00       142\n",
            "                             Intent_Comparison_LOI_Lease       1.00      1.00      1.00       140\n",
            "Intent_Comparison_LOI_Lease;Intent_Amendment_Abstraction       1.00      1.00      1.00       136\n",
            "                                Intent_Lease_Abstraction       1.00      1.00      1.00       129\n",
            "   Intent_Lease_Abstraction;Intent_Amendment_Abstraction       1.00      1.00      1.00        40\n",
            "          Intent_Lease_Abstraction;Intent_Clause_Protect       1.00      1.00      1.00       156\n",
            "                        Intent_Lease_Listings_Comparison       1.00      1.00      1.00       142\n",
            "Intent_Lease_Listings_Comparison;Intent_Company_research       1.00      1.00      1.00       127\n",
            "                        Intent_Sales_Listings_Comparison       1.00      1.00      1.00       146\n",
            "Intent_Sales_Listings_Comparison;Intent_Company_research       1.00      1.00      1.00       135\n",
            "                       Intent_Transaction_Date_navigator       1.00      1.00      1.00       138\n",
            " Intent_Transaction_Date_navigator;Intent_Clause_Protect       1.00      1.00      1.00       171\n",
            "\n",
            "                                                accuracy                           1.00      2000\n",
            "                                               macro avg       1.00      1.00      1.00      2000\n",
            "                                            weighted avg       1.00      1.00      1.00      2000\n",
            "\n",
            "\n",
            "Saving to Google Drive...\n",
            "✅ Model saved to Google Drive at: /content/drive/MyDrive/email_intent_models/email_intent_model_20250512_170603\n",
            "\n",
            "🎉 Training completed successfully!\n",
            "Model: email_intent_model_20250512_170603\n",
            "Location: /content/drive/MyDrive/email_intent_models/email_intent_model_20250512_170603\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# This script is designed to run ONLY in Google Colab\n",
        "# Please copy and paste this into a Colab notebook cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# Upload your dataset to Colab if needed\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Uncomment if you need to upload dataset\n",
        "\n",
        "# Load and prepare data\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('/content/Email_Intent_recognition/email_intent_dataset.csv')\n",
        "df = df[['text', 'intent']]\n",
        "df = df.dropna()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['intent'])\n",
        "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Create and tokenize dataset\n",
        "dataset = Dataset.from_pandas(df[['text', 'label']])\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Initialize model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    save_steps=500\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    # save_strategy=\"epoch\",\n",
        "    # load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Define compute metrics\n",
        "def compute_metrics(p):\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(p.label_ids, preds),\n",
        "        'f1': f1_score(p.label_ids, preds, average='weighted')\n",
        "    }\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Training model...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate and save results\n",
        "predictions = trainer.predict(tokenized_dataset['test'])\n",
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Save model locally first\n",
        "model.save_pretrained('./results')\n",
        "tokenizer.save_pretrained('./results')\n",
        "\n",
        "# Save label mapping\n",
        "with open('./results/label_mapping.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label,\n",
        "        'label_classes': list(label_encoder.classes_)\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Create timestamp for unique folder naming\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_name = f\"email_intent_model_{timestamp}\"\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    'model_name': model_name,\n",
        "    'training_timestamp': timestamp,\n",
        "    'dataset_size': len(df),\n",
        "    'num_labels': len(label2id),\n",
        "    'train_size': len(tokenized_dataset['train']),\n",
        "    'test_size': len(tokenized_dataset['test']),\n",
        "    'label_classes': list(label_encoder.classes_)\n",
        "}\n",
        "\n",
        "with open('./results/training_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# Save classification report\n",
        "classification_report_dict = classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=label_encoder.classes_,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "with open('./results/classification_report.json', 'w') as f:\n",
        "    json.dump(classification_report_dict, f, indent=2)\n",
        "\n",
        "# Save to Google Drive\n",
        "print(\"\\nSaving to Google Drive...\")\n",
        "gdrive_path = f'/content/drive/MyDrive/email_intent_models/{model_name}'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(gdrive_path), exist_ok=True)\n",
        "\n",
        "# Copy the entire results folder to Google Drive\n",
        "if os.path.exists(gdrive_path):\n",
        "    shutil.rmtree(gdrive_path)\n",
        "shutil.copytree('./results', gdrive_path)\n",
        "\n",
        "print(f\"✅ Model saved to Google Drive at: {gdrive_path}\")\n",
        "\n",
        "# Create a quick reference file with latest model info\n",
        "with open(f'/content/drive/MyDrive/email_intent_models/latest_model.txt', 'w') as f:\n",
        "    f.write(f\"Latest trained model: {model_name}\\n\")\n",
        "    f.write(f\"Path: {gdrive_path}\\n\")\n",
        "    f.write(f\"Training completed: {timestamp}\\n\")\n",
        "    f.write(f\"Accuracy: {classification_report_dict['accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Number of classes: {len(label_encoder.classes_)}\\n\")\n",
        "\n",
        "print(f\"\\n🎉 Training completed successfully!\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Location: {gdrive_path}\")\n",
        "print(f\"Accuracy: {classification_report_dict['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ8MXYPYeXKJ",
        "outputId": "7bfa481f-9694-4adc-9f49-9f818f30eb73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-12 17:15:07.019238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747070107.053759   27453 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747070107.063140   27453 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Email_Intent_recognition/Inference_test.py\", line 191, in <module>\n",
            "    drive.mount('/content/drive')\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 100, in mount\n",
            "    return _mount(\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 137, in _mount\n",
            "    _message.blocking_request(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 173, in blocking_request\n",
            "    request_id = send_request(\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 117, in send_request\n",
            "    instance = ipython.get_kernelapp()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_ipython.py\", line 28, in get_kernelapp\n",
            "    return get_ipython().kernel.parent\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'kernel'\n"
          ]
        }
      ],
      "source": [
        "!python Inference_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "doW5NdEDfgX3",
        "outputId": "26d56220-b5c9-420d-af60-128e64db1ddd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantVocabTranslatorCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from ...modeling_attn_mask_utils import (\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.strings'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c8ef2dd2d70c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.strings'"
          ]
        }
      ],
      "source": [
        "# Interactive inference script for Google Colab\n",
        "# Use this for testing individual emails interactively\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to load model from Google Drive\n",
        "def load_model_from_gdrive():\n",
        "    \"\"\"Load the latest model from Google Drive\"\"\"\n",
        "\n",
        "    # Find latest model\n",
        "    latest_model_file = '/content/drive/MyDrive/email_intent_models/latest_model.txt'\n",
        "    models_dir = '/content/drive/MyDrive/email_intent_models'\n",
        "\n",
        "    model_path = None\n",
        "\n",
        "    # Try to read latest model info\n",
        "    if os.path.exists(latest_model_file):\n",
        "        try:\n",
        "            with open(latest_model_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    if line.startswith('Path: '):\n",
        "                        model_path = line.split('Path: ')[1].strip()\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # If not found, look for any model directory\n",
        "    if model_path is None or not os.path.exists(model_path):\n",
        "        if os.path.exists(models_dir):\n",
        "            models = [d for d in os.listdir(models_dir)\n",
        "                     if os.path.isdir(os.path.join(models_dir, d)) and 'email_intent_model' in d]\n",
        "            if models:\n",
        "                models.sort(reverse=True)\n",
        "                model_path = os.path.join(models_dir, models[0])\n",
        "\n",
        "    if model_path is None or not os.path.exists(model_path):\n",
        "        raise Exception(\"No trained model found in Google Drive\")\n",
        "\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    # Load metadata\n",
        "    is_multi_label = False\n",
        "    intent_to_id = {}\n",
        "    id_to_intent = {}\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    # Try to load metadata.pkl first\n",
        "    metadata_path = os.path.join(model_path, 'metadata.pkl')\n",
        "    if os.path.exists(metadata_path):\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "\n",
        "        is_multi_label = metadata['is_multi_label']\n",
        "        intent_to_id = metadata['intent_to_id']\n",
        "        id_to_intent = metadata['id_to_intent']\n",
        "\n",
        "        if is_multi_label:\n",
        "            best_threshold = metadata.get('best_threshold', 0.5)\n",
        "    else:\n",
        "        # Try to load label_mapping.json\n",
        "        label_mapping_path = os.path.join(model_path, 'label_mapping.json')\n",
        "        if os.path.exists(label_mapping_path):\n",
        "            with open(label_mapping_path, 'r') as f:\n",
        "                label_mapping = json.load(f)\n",
        "\n",
        "            intent_to_id = label_mapping['label2id']\n",
        "            id_to_intent = label_mapping['id2label']\n",
        "            # Convert string keys to integers for id_to_intent\n",
        "            id_to_intent = {int(k): v for k, v in id_to_intent.items()}\n",
        "\n",
        "    return model, tokenizer, is_multi_label, id_to_intent, best_threshold\n",
        "\n",
        "# Function to predict intent\n",
        "def predict_intent(email_text, model, tokenizer, is_multi_label, id_to_intent, threshold=0.5):\n",
        "    \"\"\"Predict intent for a given email text\"\"\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(email_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Get model output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if is_multi_label:\n",
        "            # Multi-label prediction\n",
        "            probs = torch.sigmoid(logits).squeeze()\n",
        "\n",
        "            predicted_indices = (probs > threshold).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            # If no intents exceed threshold, take the top one\n",
        "            if len(predicted_indices) == 0:\n",
        "                predicted_indices = [torch.argmax(probs).item()]\n",
        "\n",
        "            # Get predicted intents and their probabilities\n",
        "            results = []\n",
        "            for idx in predicted_indices:\n",
        "                intent_name = id_to_intent[idx.item()]\n",
        "                probability = probs[idx].item()\n",
        "                results.append((intent_name, probability))\n",
        "\n",
        "            # Sort by probability (highest first)\n",
        "            results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return results\n",
        "        else:\n",
        "            # Single-label prediction\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
        "            pred_label_id = torch.argmax(probs, dim=0).item()\n",
        "            confidence = probs[pred_label_id].item()\n",
        "\n",
        "            predicted_intent = id_to_intent[pred_label_id]\n",
        "\n",
        "            return [(predicted_intent, confidence)]\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model from Google Drive...\")\n",
        "model, tokenizer, is_multi_label, id_to_intent, best_threshold = load_model_from_gdrive()\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model type: {'Multi-intent' if is_multi_label else 'Single-intent'}\")\n",
        "print(f\"Number of intents: {len(id_to_intent)}\")\n",
        "if is_multi_label:\n",
        "    print(f\"Best threshold: {best_threshold}\")\n",
        "\n",
        "# Test with example emails\n",
        "test_emails = [\n",
        "    \"Please abstract the lease for the Johnson project (PDF attached). We need to know the base rent, commencement and expiry dates, renewal options, and escalation schedule.\",\n",
        "    \"Compare the signed lease to the LOI we submitted last month. I want to know what terms got changed or added, especially around TI allowances.\",\n",
        "    \"Could you do a background check on Wexford Corp before we proceed? I'm particularly interested in any public disputes or bankruptcies in the past 5 years.\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== Testing with sample emails ===\")\n",
        "for i, email in enumerate(test_emails, 1):\n",
        "    print(f\"\\nTest {i}:\")\n",
        "    print(f\"Email: {email[:60]}...\")\n",
        "\n",
        "    results = predict_intent(email, model, tokenizer, is_multi_label, id_to_intent, best_threshold)\n",
        "\n",
        "    if is_multi_label:\n",
        "        print(\"Predicted Intents:\")\n",
        "        for intent, confidence in results:\n",
        "            print(f\"  - {intent}: {confidence:.4f}\")\n",
        "    else:\n",
        "        intent, confidence = results[0]\n",
        "        print(f\"Predicted Intent: {intent}\")\n",
        "        print(f\"Confidence: {confidence:.4f}\")\n",
        "\n",
        "# Interactive prediction function\n",
        "def predict_email_intent(email_text):\n",
        "    \"\"\"Predict intent for any email text\"\"\"\n",
        "    results = predict_intent(email_text, model, tokenizer, is_multi_label, id_to_intent, best_threshold)\n",
        "\n",
        "    if is_multi_label:\n",
        "        print(\"\\nPredicted Intents:\")\n",
        "        for intent, confidence in results:\n",
        "            print(f\"  - {intent}: {confidence:.4f}\")\n",
        "\n",
        "        # Format for API output\n",
        "        intent_strings = [intent for intent, _ in results]\n",
        "        avg_confidence = np.mean([conf for _, conf in results])\n",
        "        return \";\".join(intent_strings), avg_confidence\n",
        "    else:\n",
        "        intent, confidence = results[0]\n",
        "        print(f\"\\nPredicted Intent: {intent}\")\n",
        "        print(f\"Confidence: {confidence:.4f}\")\n",
        "        return intent, confidence\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Ready for inference!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "To predict intent for any email, use:\n",
        "    predict_email_intent(\"Your email text here...\")\n",
        "\n",
        "Example:\n",
        "    predict_email_intent(\"Can you extract the key dates from the lease document?\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUox4wkclYlU"
      },
      "source": [
        "## Inference From GC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TALMChJWlXZX",
        "outputId": "742449b8-58be-479b-b5cd-4f14c7b50781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Intent_Lease_Abstraction|0.8695\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# Inference.py - Compatible with app.py and loads from Google Drive\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "def mount_gdrive():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        # Not in Colab, assume Google Drive is accessible\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        # Already mounted\n",
        "        return True\n",
        "\n",
        "# Check command line arguments\n",
        "if len(sys.argv) < 2:\n",
        "    print(\"ERROR: No email text provided\")\n",
        "    sys.exit(1)\n",
        "\n",
        "email_text = sys.argv[1]\n",
        "\n",
        "# Mount Google Drive\n",
        "mount_gdrive()\n",
        "\n",
        "# Find the latest model in Google Drive\n",
        "def find_latest_model():\n",
        "    \"\"\"Find the latest trained model in Google Drive\"\"\"\n",
        "    # Check for latest model reference file\n",
        "    latest_model_file = '/content/drive/MyDrive/email_intent_models/latest_model.txt'\n",
        "    models_dir = '/content/drive/MyDrive/email_intent_models'\n",
        "\n",
        "    model_path = None\n",
        "\n",
        "    # Try to read latest model info\n",
        "    if os.path.exists(latest_model_file):\n",
        "        try:\n",
        "            with open(latest_model_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    if line.startswith('Path: '):\n",
        "                        model_path = line.split('Path: ')[1].strip()\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # If not found, look for any model directory\n",
        "    if model_path is None or not os.path.exists(model_path):\n",
        "        if os.path.exists(models_dir):\n",
        "            models = [d for d in os.listdir(models_dir)\n",
        "                     if os.path.isdir(os.path.join(models_dir, d)) and 'email_intent_model' in d]\n",
        "            if models:\n",
        "                # Get the most recent model (sorted alphabetically, which includes timestamp)\n",
        "                models.sort(reverse=True)\n",
        "                model_path = os.path.join(models_dir, models[0])\n",
        "\n",
        "    return model_path\n",
        "\n",
        "# Find and load the model\n",
        "model_path = find_latest_model()\n",
        "\n",
        "if model_path is None or not os.path.exists(model_path):\n",
        "    print(\"ERROR: No trained model found in Google Drive\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load model and tokenizer\n",
        "try:\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load model - {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load metadata for label mapping\n",
        "try:\n",
        "    # Try to load metadata.pkl first (for multi-intent models)\n",
        "    metadata_path = os.path.join(model_path, 'metadata.pkl')\n",
        "    if os.path.exists(metadata_path):\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "\n",
        "        is_multi_label = metadata['is_multi_label']\n",
        "        intent_to_id = metadata['intent_to_id']\n",
        "        id_to_intent = metadata['id_to_intent']\n",
        "\n",
        "        if is_multi_label:\n",
        "            best_threshold = metadata.get('best_threshold', 0.5)\n",
        "\n",
        "    else:\n",
        "        # Try to load label_mapping.json (for single-intent models)\n",
        "        label_mapping_path = os.path.join(model_path, 'label_mapping.json')\n",
        "        if os.path.exists(label_mapping_path):\n",
        "            with open(label_mapping_path, 'r') as f:\n",
        "                label_mapping = json.load(f)\n",
        "\n",
        "            # Assume single-label if no metadata.pkl\n",
        "            is_multi_label = False\n",
        "            intent_to_id = label_mapping['label2id']\n",
        "            id_to_intent = label_mapping['id2label']\n",
        "            # Convert string keys to integers for id_to_intent\n",
        "            id_to_intent = {int(k): v for k, v in id_to_intent.items()}\n",
        "        else:\n",
        "            print(\"ERROR: No metadata or label mapping found\")\n",
        "            sys.exit(1)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load metadata - {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Predict intent\n",
        "try:\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(email_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Get model output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if is_multi_label:\n",
        "            # Multi-label prediction\n",
        "            probs = torch.sigmoid(logits).squeeze()\n",
        "\n",
        "            # Use best threshold or default\n",
        "            threshold = best_threshold if 'best_threshold' in locals() else 0.5\n",
        "            predicted_indices = (probs > threshold).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            # If no intents exceed threshold, take the top one\n",
        "            if len(predicted_indices) == 0:\n",
        "                predicted_indices = [torch.argmax(probs).item()]\n",
        "\n",
        "            # Get predicted intents and their probabilities\n",
        "            predicted_intents = []\n",
        "            intent_probabilities = []\n",
        "\n",
        "            for idx in predicted_indices:\n",
        "                intent_name = id_to_intent[idx.item()]\n",
        "                probability = probs[idx].item()\n",
        "                predicted_intents.append(intent_name)\n",
        "                intent_probabilities.append(probability)\n",
        "\n",
        "            # Sort by probability (highest first)\n",
        "            paired_results = list(zip(predicted_intents, intent_probabilities))\n",
        "            paired_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Format output - join multiple intents with semicolon\n",
        "            intent_strings = [intent for intent, _ in paired_results]\n",
        "            result_intent = \";\".join(intent_strings)\n",
        "\n",
        "            # Calculate average confidence for multi-intent\n",
        "            avg_confidence = np.mean([prob for _, prob in paired_results])\n",
        "\n",
        "            # Return result in expected format for app.py\n",
        "            print(f\"{result_intent}|{avg_confidence:.4f}\")\n",
        "\n",
        "        else:\n",
        "            # Single-label prediction\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
        "            pred_label_id = torch.argmax(probs, dim=0).item()\n",
        "            confidence = probs[pred_label_id].item()\n",
        "\n",
        "            # Get predicted intent\n",
        "            predicted_intent = id_to_intent[pred_label_id]\n",
        "\n",
        "            # Return result in expected format for app.py\n",
        "            print(f\"{predicted_intent}|{confidence:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Prediction failed - {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijwT2vMFmE2Y",
        "outputId": "44efa7b0-730c-47ba-fed1-64b6d58af97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ngrok ...\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-4fb0df585ebc>:144: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Your public URL: NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\n",
            "==================================================\n",
            "API ENDPOINTS:\n",
            "==================================================\n",
            "GET  NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"/ - API status\n",
            "POST NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"/predict - Predict email intent\n",
            "GET  NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"/health - Health check\n",
            "GET  NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"/model-info - Current model information\n",
            "GET  NgrokTunnel: \"https://9659-34-87-66-138.ngrok-free.app\" -> \"http://localhost:8000\"/docs - Interactive API documentation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [6983]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     223.233.70.49:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     223.233.70.49:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     223.233.70.49:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import subprocess\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# Apply nest_asyncio to allow uvicorn.run inside notebooks or scripts\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create the FastAPI app\n",
        "app = FastAPI(title=\"Email Intent Classification API\", version=\"1.0.0\")\n",
        "\n",
        "# Define input model\n",
        "class EmailText(BaseModel):\n",
        "    email_text: str\n",
        "\n",
        "# Define response model\n",
        "class PredictionResponse(BaseModel):\n",
        "    email_text: str\n",
        "    predicted_intent: str\n",
        "    confidence: float\n",
        "    is_multi_intent: bool = False\n",
        "    all_intents: list = []\n",
        "\n",
        "# Initialize check - verify Inference.py exists\n",
        "def check_inference_script():\n",
        "    \"\"\"Check if Inference.py exists and is accessible\"\"\"\n",
        "    if not os.path.exists(\"Inference.py\"):\n",
        "        print(\"WARNING: Inference.py not found in current directory\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Define the /predict endpoint\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict_intent(data: EmailText):\n",
        "    try:\n",
        "        # Call Inference.py with the email text as argument\n",
        "        result = subprocess.run(\n",
        "            [\"python3\", \"Inference.py\", data.email_text],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "\n",
        "        output = result.stdout.strip()\n",
        "\n",
        "        # Handle both single and multi-intent outputs\n",
        "        if \"|\" in output:\n",
        "            predicted_intent, confidence = output.split(\"|\")\n",
        "            confidence = float(confidence)\n",
        "\n",
        "            # Check if it's multi-intent (contains semicolons)\n",
        "            if \";\" in predicted_intent:\n",
        "                # Multi-intent case\n",
        "                intents = predicted_intent.split(\";\")\n",
        "                return PredictionResponse(\n",
        "                    email_text=data.email_text,\n",
        "                    predicted_intent=predicted_intent,  # Full string with semicolons\n",
        "                    confidence=confidence,\n",
        "                    is_multi_intent=True,\n",
        "                    all_intents=intents\n",
        "                )\n",
        "            else:\n",
        "                # Single intent case\n",
        "                return PredictionResponse(\n",
        "                    email_text=data.email_text,\n",
        "                    predicted_intent=predicted_intent,\n",
        "                    confidence=confidence,\n",
        "                    is_multi_intent=False,\n",
        "                    all_intents=[predicted_intent]\n",
        "                )\n",
        "        else:\n",
        "            return {\"error\": \"Invalid output format from inference script\", \"raw_output\": output}\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error_msg = e.stderr.strip() if e.stderr else \"Unknown error\"\n",
        "        return {\"error\": \"Prediction script failed\", \"details\": error_msg}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Root endpoint\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\n",
        "        \"message\": \"Email Intent Classification API\",\n",
        "        \"status\": \"running\",\n",
        "        \"endpoints\": {\n",
        "            \"/predict\": \"POST - Predict intent for email text\",\n",
        "            \"/docs\": \"GET - API documentation\",\n",
        "            \"/health\": \"GET - Health check\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    \"\"\"Check if the API and inference script are working\"\"\"\n",
        "    try:\n",
        "        # Test if Inference.py can be called\n",
        "        result = subprocess.run(\n",
        "            [\"python3\", \"-c\", \"import Inference\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            return {\n",
        "                \"status\": \"healthy\",\n",
        "                \"inference_script\": \"accessible\",\n",
        "                \"message\": \"API is ready to make predictions\"\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"status\": \"warning\",\n",
        "                \"inference_script\": \"error\",\n",
        "                \"message\": \"Inference script has issues\",\n",
        "                \"details\": result.stderr.strip()\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"status\": \"error\",\n",
        "            \"message\": str(e)\n",
        "        }\n",
        "\n",
        "# Model info endpoint\n",
        "@app.get(\"/model-info\")\n",
        "def model_info():\n",
        "    \"\"\"Get information about the current model\"\"\"\n",
        "    try:\n",
        "        # Try to get model info from Google Drive\n",
        "        latest_model_file = '/content/drive/MyDrive/email_intent_models/latest_model.txt'\n",
        "        if os.path.exists(latest_model_file):\n",
        "            with open(latest_model_file, 'r') as f:\n",
        "                content = f.read()\n",
        "            return {\"model_info\": content, \"source\": \"Google Drive\"}\n",
        "        else:\n",
        "            return {\"message\": \"No model info available\", \"source\": \"Google Drive not accessible\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Startup event\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"Run checks when the API starts\"\"\"\n",
        "    print(\"🚀 Starting Email Intent Classification API...\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"✅ Google Drive mounted successfully\")\n",
        "    except:\n",
        "        print(\"⚠️ Google Drive mounting failed or not in Colab\")\n",
        "\n",
        "    # Check if Inference.py exists\n",
        "    if check_inference_script():\n",
        "        print(\"✅ Inference script found\")\n",
        "    else:\n",
        "        print(\"❌ Inference script not found\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set ngrok authtoken\n",
        "    ngrok.set_auth_token(\"2ljGDXBe5aMRPGsbmCMZCkBXAoB_43ekyReP1bEYvLmLgfcPE\")\n",
        "\n",
        "    # Start ngrok tunnel on port 8000\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"🚀 Your public URL:\", public_url)\n",
        "\n",
        "    # Print API endpoints\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"API ENDPOINTS:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"GET  {public_url}/ - API status\")\n",
        "    print(f\"POST {public_url}/predict - Predict email intent\")\n",
        "    print(f\"GET  {public_url}/health - Health check\")\n",
        "    print(f\"GET  {public_url}/model-info - Current model information\")\n",
        "    print(f\"GET  {public_url}/docs - Interactive API documentation\")\n",
        "\n",
        "    # Run the FastAPI app\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# startup.py - Run this before starting the API\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive first\n",
        "def mount_google_drive():\n",
        "    \"\"\"Mount Google Drive in the current session\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"✅ Google Drive mounted successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Google Drive mounting failed: {e}\")\n",
        "        # Check if it's already mounted\n",
        "        if os.path.exists('/content/drive/MyDrive'):\n",
        "            print(\"✅ Google Drive appears to be already mounted\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Check if model exists\n",
        "def check_model_exists():\n",
        "    \"\"\"Check if model exists in Google Drive\"\"\"\n",
        "    latest_model_file = '/content/drive/MyDrive/email_intent_models/latest_model.txt'\n",
        "    models_dir = '/content/drive/MyDrive/email_intent_models'\n",
        "\n",
        "    if os.path.exists(latest_model_file):\n",
        "        print(\"✅ Latest model reference found\")\n",
        "        return True\n",
        "    elif os.path.exists(models_dir):\n",
        "        models = [d for d in os.listdir(models_dir)\n",
        "                 if os.path.isdir(os.path.join(models_dir, d)) and 'email_intent_model' in d]\n",
        "        if models:\n",
        "            print(f\"✅ Found {len(models)} model(s) in Google Drive\")\n",
        "            return True\n",
        "\n",
        "    print(\"❌ No models found in Google Drive\")\n",
        "    return False\n",
        "\n",
        "# Test inference script\n",
        "def test_inference_script():\n",
        "    \"\"\"Test if the inference script works\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"python\", \"Inference_test.py\", \"Test email for inference\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ Inference script test successful\")\n",
        "            print(f\"Output: {result.stdout.strip()}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ Inference script test failed\")\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "            return False\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"⚠️ Inference script test timed out\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error testing inference script: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main setup function\n",
        "def setup_api():\n",
        "    \"\"\"Setup everything needed for the API\"\"\"\n",
        "    print(\"🚀 Setting up Email Intent Classification API...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Step 1: Mount Google Drive\n",
        "    print(\"1. Mounting Google Drive...\")\n",
        "    if not mount_google_drive():\n",
        "        print(\"❌ Cannot proceed without Google Drive access\")\n",
        "        return False\n",
        "\n",
        "    # Step 2: Check model exists\n",
        "    print(\"\\n2. Checking for trained models...\")\n",
        "    if not check_model_exists():\n",
        "        print(\"❌ No trained models found. Please train a model first.\")\n",
        "        return False\n",
        "\n",
        "    # Step 3: Test inference script\n",
        "    print(\"\\n3. Testing inference script...\")\n",
        "    if not test_inference_script():\n",
        "        print(\"❌ Inference script is not working properly\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✅ Setup completed successfully!\")\n",
        "    print(\"You can now start the API with: python3 app.py\")\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = setup_api()\n",
        "    sys.exit(0 if success else 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "eihmOwZlsMnH",
        "outputId": "65726211-a6b0-4435-b603-dfaf9f8534df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Setting up Email Intent Classification API...\n",
            "==================================================\n",
            "1. Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully\n",
            "\n",
            "2. Checking for trained models...\n",
            "✅ Latest model reference found\n",
            "\n",
            "3. Testing inference script...\n",
            "✅ Inference script test successful\n",
            "Output: Intent_Lease_Listings_Comparison|0.4573\n",
            "\n",
            "==================================================\n",
            "✅ Setup completed successfully!\n",
            "You can now start the API with: python3 app.py\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import subprocess\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Apply nest_asyncio to allow uvicorn.run inside notebooks or scripts\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create the FastAPI app\n",
        "app = FastAPI(title=\"Email Intent Classification API\", version=\"1.0.0\")\n",
        "\n",
        "# Define input model\n",
        "class EmailText(BaseModel):\n",
        "    email_text: str\n",
        "\n",
        "# Define response model\n",
        "class PredictionResponse(BaseModel):\n",
        "    email_text: str\n",
        "    predicted_intent: str\n",
        "    confidence: float\n",
        "    is_multi_intent: bool = False\n",
        "    all_intents: list = []\n",
        "\n",
        "# Startup checks\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"Run checks when the API starts\"\"\"\n",
        "    print(\"🚀 Starting Email Intent Classification API...\")\n",
        "\n",
        "    # Check if Google Drive is accessible\n",
        "    if not os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"⚠️ Google Drive not accessible. Attempting to mount...\")\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "            print(\"✅ Google Drive mounted successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to mount Google Drive: {e}\")\n",
        "    else:\n",
        "        print(\"✅ Google Drive is accessible\")\n",
        "\n",
        "    # Check if Inference.py exists\n",
        "    if not os.path.exists(\"Inference.py\"):\n",
        "        print(\"❌ Inference.py not found\")\n",
        "    else:\n",
        "        print(\"✅ Inference.py found\")\n",
        "\n",
        "# Define the /predict endpoint\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict_intent(data: EmailText):\n",
        "    try:\n",
        "        # Set environment variables to reduce verbosity\n",
        "        env = os.environ.copy()\n",
        "        env['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "        env['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU usage to avoid CUDA issues\n",
        "\n",
        "        # Call Inference.py with the email text as argument\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"Inference.py\", data.email_text],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            env=env,\n",
        "            timeout=30  # 30 second timeout\n",
        "        )\n",
        "\n",
        "        output = result.stdout.strip()\n",
        "\n",
        "        # Handle both single and multi-intent outputs\n",
        "        if \"|\" in output:\n",
        "            predicted_intent, confidence = output.split(\"|\")\n",
        "            confidence = float(confidence)\n",
        "\n",
        "            # Check if it's multi-intent (contains semicolons)\n",
        "            if \";\" in predicted_intent:\n",
        "                # Multi-intent case\n",
        "                intents = predicted_intent.split(\";\")\n",
        "                return PredictionResponse(\n",
        "                    email_text=data.email_text,\n",
        "                    predicted_intent=predicted_intent,  # Full string with semicolons\n",
        "                    confidence=confidence,\n",
        "                    is_multi_intent=True,\n",
        "                    all_intents=intents\n",
        "                )\n",
        "            else:\n",
        "                # Single intent case\n",
        "                return PredictionResponse(\n",
        "                    email_text=data.email_text,\n",
        "                    predicted_intent=predicted_intent,\n",
        "                    confidence=confidence,\n",
        "                    is_multi_intent=False,\n",
        "                    all_intents=[predicted_intent]\n",
        "                )\n",
        "        else:\n",
        "            raise HTTPException(\n",
        "                status_code=500,\n",
        "                detail=f\"Invalid output format from inference script: {output}\"\n",
        "            )\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        raise HTTPException(\n",
        "            status_code=408,\n",
        "            detail=\"Inference script timed out\"\n",
        "        )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error_msg = e.stderr.strip() if e.stderr else \"Unknown error\"\n",
        "        # Filter out CUDA warnings from error message\n",
        "        error_lines = error_msg.split('\\n')\n",
        "        filtered_errors = [line for line in error_lines\n",
        "                          if not any(keyword in line for keyword in ['cuda', 'CUDA', 'cuDNN', 'cuBLAS', 'cuFFT'])]\n",
        "        filtered_error = '\\n'.join(filtered_errors).strip()\n",
        "\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Prediction script failed: {filtered_error}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=str(e)\n",
        "        )\n",
        "\n",
        "# Root endpoint\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\n",
        "        \"message\": \"Email Intent Classification API\",\n",
        "        \"status\": \"running\",\n",
        "        \"endpoints\": {\n",
        "            \"/predict\": \"POST - Predict intent for email text\",\n",
        "            \"/docs\": \"GET - API documentation\",\n",
        "            \"/health\": \"GET - Health check\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    \"\"\"Check if the API and inference script are working\"\"\"\n",
        "    checks = {\n",
        "        \"api\": \"healthy\",\n",
        "        \"google_drive\": \"unknown\",\n",
        "        \"inference_script\": \"unknown\",\n",
        "        \"model\": \"unknown\"\n",
        "    }\n",
        "\n",
        "    # Check Google Drive\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        checks[\"google_drive\"] = \"accessible\"\n",
        "    else:\n",
        "        checks[\"google_drive\"] = \"not accessible\"\n",
        "\n",
        "    # Check Inference.py\n",
        "    if os.path.exists(\"Inference.py\"):\n",
        "        checks[\"inference_script\"] = \"found\"\n",
        "    else:\n",
        "        checks[\"inference_script\"] = \"not found\"\n",
        "\n",
        "    # Check if model exists\n",
        "    if os.path.exists('/content/drive/MyDrive/email_intent_models'):\n",
        "        checks[\"model\"] = \"directory found\"\n",
        "    else:\n",
        "        checks[\"model\"] = \"directory not found\"\n",
        "\n",
        "    # Determine overall status\n",
        "    if checks[\"google_drive\"] == \"accessible\" and checks[\"inference_script\"] == \"found\":\n",
        "        status = \"healthy\"\n",
        "    else:\n",
        "        status = \"degraded\"\n",
        "\n",
        "    return {\n",
        "        \"status\": status,\n",
        "        \"checks\": checks,\n",
        "        \"message\": \"Health check completed\"\n",
        "    }\n",
        "\n",
        "# Model info endpoint\n",
        "@app.get(\"/model-info\")\n",
        "def model_info():\n",
        "    \"\"\"Get information about the current model\"\"\"\n",
        "    try:\n",
        "        latest_model_file = '/content/drive/MyDrive/email_intent_models/latest_model.txt'\n",
        "        if os.path.exists(latest_model_file):\n",
        "            with open(latest_model_file, 'r') as f:\n",
        "                content = f.read()\n",
        "            return {\"model_info\": content, \"source\": \"Google Drive\"}\n",
        "        else:\n",
        "            return {\"message\": \"No model info available\", \"source\": \"Google Drive not accessible\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set ngrok authtoken\n",
        "    ngrok.set_auth_token(\"2ljGDXBe5aMRPGsbmCMZCkBXAoB_43ekyReP1bEYvLmLgfcPE\")\n",
        "\n",
        "    # Start ngrok tunnel on port 8000\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"🚀 Your public URL:\", public_url)\n",
        "\n",
        "    # Print API endpoints\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"API ENDPOINTS:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"GET  {public_url}/ - API status\")\n",
        "    print(f\"POST {public_url}/predict - Predict email intent\")\n",
        "    print(f\"GET  {public_url}/health - Health check\")\n",
        "    print(f\"GET  {public_url}/model-info - Current model information\")\n",
        "    print(f\"GET  {public_url}/docs - Interactive API documentation\")\n",
        "\n",
        "    # Run the FastAPI app\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGaHxIv_rItO",
        "outputId": "d861af11-63a1-49c1-eb33-508fb8b76a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3083f4ea33af>:29: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Your public URL: NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\n",
            "==================================================\n",
            "API ENDPOINTS:\n",
            "==================================================\n",
            "GET  NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"/ - API status\n",
            "POST NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"/predict - Predict email intent\n",
            "GET  NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"/health - Health check\n",
            "GET  NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"/model-info - Current model information\n",
            "GET  NgrokTunnel: \"https://cfa2-34-125-240-16.ngrok-free.app\" -> \"http://localhost:8000\"/docs - Interactive API documentation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [510]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     223.233.70.49:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     223.233.70.49:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     223.233.70.49:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [510]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08a53fa6786944629d60a2937172a370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66d41fc8cb142309f1d4d34ca9d710e",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57edf8af70684220ba96e521cf9c1082",
            "value": 8000
          }
        },
        "3a9b6f271082492b9703fe6ddda2b850": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd129c2e3da94774a9736a16e8c4721d",
            "placeholder": "​",
            "style": "IPY_MODEL_9da1159d7c8a41bebe061b58e03db0b4",
            "value": " 2000/2000 [00:00&lt;00:00, 2638.38 examples/s]"
          }
        },
        "57edf8af70684220ba96e521cf9c1082": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b073226c8194df8b2fe74fd2418184c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfab7bf25cb04af99cb8a640eeda571d",
              "IPY_MODEL_08a53fa6786944629d60a2937172a370",
              "IPY_MODEL_f9f42e7d88d84ccdbab419720268099d"
            ],
            "layout": "IPY_MODEL_cf0f2721567448a0b1740ae3ea046671"
          }
        },
        "5f296bbb606a44b3bfddc0bc38dcf252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec2666c4a37540e08b9edea94c0826e0",
            "placeholder": "​",
            "style": "IPY_MODEL_77fe1ae1e8344882ad929da9835fc765",
            "value": "Map: 100%"
          }
        },
        "77fe1ae1e8344882ad929da9835fc765": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b684c4651340daa837e5ab4bb362f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ab13dd36aff41a992b55970439a39ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da1159d7c8a41bebe061b58e03db0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e4f8e33dc0743baa56efe35c7523c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a25983eb77c1424188a833c2c1235e67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a643e5c82ff04cdcbdae012ee94f55dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f296bbb606a44b3bfddc0bc38dcf252",
              "IPY_MODEL_bf54cc60cbc840a79bcac2154564357e",
              "IPY_MODEL_3a9b6f271082492b9703fe6ddda2b850"
            ],
            "layout": "IPY_MODEL_f97b9acc98224159a190399a3c2a1bd4"
          }
        },
        "bf54cc60cbc840a79bcac2154564357e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a25983eb77c1424188a833c2c1235e67",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79b684c4651340daa837e5ab4bb362f3",
            "value": 2000
          }
        },
        "cf0f2721567448a0b1740ae3ea046671": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfab7bf25cb04af99cb8a640eeda571d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab13dd36aff41a992b55970439a39ad",
            "placeholder": "​",
            "style": "IPY_MODEL_f64cff16376440a28e4c85b277b8366b",
            "value": "Map: 100%"
          }
        },
        "da1c89788ee9412499de080d6aa67d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec2666c4a37540e08b9edea94c0826e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f64cff16376440a28e4c85b277b8366b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66d41fc8cb142309f1d4d34ca9d710e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97b9acc98224159a190399a3c2a1bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9f42e7d88d84ccdbab419720268099d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1c89788ee9412499de080d6aa67d8b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e4f8e33dc0743baa56efe35c7523c50",
            "value": " 8000/8000 [00:03&lt;00:00, 2524.49 examples/s]"
          }
        },
        "fd129c2e3da94774a9736a16e8c4721d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}